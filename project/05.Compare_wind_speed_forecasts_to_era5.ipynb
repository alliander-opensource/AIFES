{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cufflinks\n",
    "cufflinks.go_offline()\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime, date\n",
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from openstef.pipeline.train_create_forecast_backtest import train_model_and_forecast_back_test\n",
    "from openstef.metrics.figure import plot_feature_importance\n",
    "from openstef.data_classes.model_specifications import ModelSpecificationDataClass\n",
    "from openstef.data_classes.prediction_job import PredictionJobDataClass\n",
    "\n",
    "# Set working dir to location of this file\n",
    "os.chdir('.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load inputs\n",
    "filename = Path(\"../.data/Middenmeer-150kV.csv\")\n",
    "\n",
    "measurements = pd.read_csv(filename, delimiter=\";\", decimal=\",\")\n",
    "measurements[\"Datetime\"] = pd.to_datetime(measurements[\"Datum\"] + \" \" + measurements[\"Tijd\"])\n",
    "measurements = measurements.set_index('Datetime').tz_localize('CET', ambiguous='NaT', nonexistent='NaT').tz_convert(\"UTC\")\n",
    "# Only keep relevant columns\n",
    "measurements = measurements.iloc[:,2:-1]\n",
    "# Sum the load\n",
    "measurements['Total'] = measurements.sum(axis=1)\n",
    "# By default, only a backtest is made for the total\n",
    "target_column = 'Total'\n",
    "\n",
    "measurements.iplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictors\n",
    "predictors = pd.read_csv('../.data/predictors.csv', index_col=0, parse_dates=True)\n",
    "predictors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwd_d_2 = (\n",
    "    pd.read_parquet(\"../.data/df_dwd_iconeu_mdm150_d2.parquet\")\n",
    "    .assign(\n",
    "        wind_speed_10=lambda df: np.sqrt(df[\"u10\"] ** 2 + df[\"v10\"] ** 2),\n",
    "        wind_direction_10=lambda df: (\n",
    "            (180 / math.pi)\n",
    "            * np.arctan2(\n",
    "                df[\"u10\"] / df[\"wind_speed_10\"], df[\"v10\"] / df[\"wind_speed_10\"]\n",
    "            )\n",
    "        )\n",
    "        + 180,\n",
    "    )\n",
    "    .resample(\"15T\")\n",
    "    .interpolate(method=\"linear\")\n",
    ")\n",
    "dwd_d_2.head()\n",
    "\n",
    "df_era5 = pd.read_parquet(\"../.data/windspeed_100m_era5_MDM.parquet\").tz_localize('UTC').resample(\"15T\").interpolate(method=\"linear\")\n",
    "df_era5 = df_era5.rename(lambda x:x+'_era5', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% define properties of training/prediction. We call this a 'prediction_job'\n",
    "pj = PredictionJobDataClass(\n",
    "    id=1,\n",
    "    name=\"TestPrediction\",\n",
    "    model=\"xgb\",\n",
    "    quantiles=[0.10, 0.30, 0.50, 0.70, 0.90],\n",
    "    horizon_minutes=24 * 60,\n",
    "    resolution_minutes=15,\n",
    "    forecast_type=\"demand\",  # Note, this should become optional\n",
    "    lat=1,  # should become optional\n",
    "    lon=1,  # should become optional\n",
    "    # train_components=False, #should become optional\n",
    "    # model_type_group=None, # Note, this should become optional\n",
    "    # hyper_params={}, # Note, this should become optional\n",
    "    # feature_names=None, # Note, this should become optional\n",
    ")\n",
    "# Define backtest specs\n",
    "backtest_specs = dict(n_folds=3, training_horizons=[47.0])\n",
    "\n",
    "modelspecs = ModelSpecificationDataClass(id=pj[\"id\"])\n",
    "loadname = measurements.iloc[:, -1].name  # Var used to store the data\n",
    "\n",
    "# Specify input data, use last column of the load dataframe\n",
    "input_data = pd.DataFrame(dict(load=measurements.loc[:, loadname])).merge(\n",
    "    predictors, left_index=True, right_index=True\n",
    ")\n",
    "# Also resample to fix overlapping indices\n",
    "input_data = input_data.resample(\"15T\").mean()\n",
    "input_data = input_data.drop('windspeed_100m', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_d2 = input_data.copy()\n",
    "input_data_d2['windspeed'] = dwd_d_2['wind_speed_10']\n",
    "input_data_era5 = input_data.copy()\n",
    "input_data_era5['windspeed'] = df_era5['ws_10m_era5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Perform the backtest\n",
    "(\n",
    "    forecast,\n",
    "    model,\n",
    "    train_data,\n",
    "    validation_data,\n",
    "    test_data,\n",
    ") = train_model_and_forecast_back_test(\n",
    "    pj,\n",
    "    modelspecs=modelspecs,\n",
    "    input_data=input_data,\n",
    "    **backtest_specs,\n",
    ")\n",
    "(\n",
    "    forecast_d2,\n",
    "    model_d2,\n",
    "    train_data_d2,\n",
    "    validation_data_d2,\n",
    "    test_data_d2,\n",
    ") = train_model_and_forecast_back_test(\n",
    "    pj,\n",
    "    modelspecs=modelspecs,\n",
    "    input_data=input_data_d2,\n",
    "    **backtest_specs,\n",
    ")\n",
    "(\n",
    "    forecast_era5,\n",
    "    model_era5,\n",
    "    train_data_era5,\n",
    "    validation_data_era5,\n",
    "    test_data_era5,\n",
    ") = train_model_and_forecast_back_test(\n",
    "    pj,\n",
    "    modelspecs=modelspecs,\n",
    "    input_data=input_data_era5,\n",
    "    **backtest_specs,\n",
    ")\n",
    "# If n_folds>1, model is a list of models. In that case, only use the first model\n",
    "if backtest_specs[\"n_folds\"] > 1:\n",
    "    model = model[0]\n",
    "    model_d2 = model_d2[0]\n",
    "    model_era5 = model_era5[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs=dict(timeseries=dict())\n",
    "for horizon in set(forecast_d2.horizon):\n",
    "    fig = forecast_d2.loc[forecast_d2.horizon==horizon,['quantile_P10','quantile_P30',\n",
    "                    'quantile_P50','quantile_P70','quantile_P90','realised','forecast']].iplot(asFigure=True,\n",
    "                                                                                   title=f\"Horizon: {horizon}\")\n",
    "    fig.update_traces(\n",
    "         line=dict(color=\"green\", width=1), fill='tonexty', fillcolor='rgba(0, 255, 0, 0.1)',\n",
    "         selector=lambda x: 'quantile' in x.name and x.name != 'quantile_P10')\n",
    "    fig.update_traces(\n",
    "         line=dict(color=\"green\", width=1),\n",
    "         selector=lambda x: 'quantile_P10' == x.name)\n",
    "    fig.update_traces(\n",
    "         line=dict(color=\"red\", width=2),\n",
    "         selector=lambda x: 'realised' in x.name)\n",
    "    fig.update_traces(\n",
    "         line=dict(color=\"blue\", width=2),\n",
    "         selector=lambda x: 'forecast' in x.name)\n",
    "    fig.show()\n",
    "\n",
    "figs=dict(timeseries=dict())\n",
    "for horizon in set(forecast_era5.horizon):\n",
    "    fig = forecast_era5.loc[forecast_era5.horizon==horizon,['quantile_P10','quantile_P30',\n",
    "                    'quantile_P50','quantile_P70','quantile_P90','realised','forecast']].iplot(asFigure=True,\n",
    "                                                                                   title=f\"Horizon: {horizon}\")\n",
    "    fig.update_traces(\n",
    "         line=dict(color=\"green\", width=1), fill='tonexty', fillcolor='rgba(0, 255, 0, 0.1)',\n",
    "         selector=lambda x: 'quantile' in x.name and x.name != 'quantile_P10')\n",
    "    fig.update_traces(\n",
    "         line=dict(color=\"green\", width=1),\n",
    "         selector=lambda x: 'quantile_P10' == x.name)\n",
    "    fig.update_traces(\n",
    "         line=dict(color=\"red\", width=2),\n",
    "         selector=lambda x: 'realised' in x.name)\n",
    "    fig.update_traces(\n",
    "         line=dict(color=\"blue\", width=2),\n",
    "         selector=lambda x: 'forecast' in x.name)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare forecasts for three different windspeed input sources\n",
    "forecast['err']=forecast['realised']-forecast['forecast']\n",
    "forecast_d2['err']=forecast_d2['realised']-forecast_d2['forecast']\n",
    "forecast_era5['err']=forecast_era5['realised']-forecast_era5['forecast']\n",
    "mae = pd.concat([forecast['err'].rename('Shortest_leadtime'),forecast_d2['err'].rename('D-2 forecast'),forecast_era5['err'].rename('Era5')], axis=1).abs().mean()\n",
    "mae_fig = mae.iplot(kind='bar',\n",
    "          layout=dict(title='MAE',\n",
    "                      xaxis=dict(title='model'),\n",
    "                      yaxis=dict(title='MAE [MW]')), asFigure=True)\n",
    "mae_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check_weather_data = pd.concat([pd.concat(test_data_d2)[['windspeed_100mExtrapolated','windspeed']], df_era5[['ws_10m_era5','ws_100m_era5']]], axis=1).dropna()\n",
    "df_check_weather_data['day'] = df_check_weather_data.index.dayofyear\n",
    "df_check_weather_data.loc[df_check_weather_data.index.minute == 0].plot.scatter(x = 'windspeed',y ='ws_10m_era5', c = 'day', colormap = 'BrBG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check_weather_data['error_10m_windspeed_forecast'] = (df_check_weather_data['windspeed'] - df_check_weather_data['ws_10m_era5'])\n",
    "df_compare_errors = pd.concat([df_check_weather_data, forecast_d2['err']],axis=1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some correlation between windforecast error and model error is clear, however the error correlates just as strong with windspeed in general\n",
    "df_compare_errors.plot.scatter(x = 'error_10m_windspeed_forecast', y = 'err', c = 'day', colormap = 'BrBG')\n",
    "df_compare_errors.loc[~np.isin(df_compare_errors.index.date, [date(2022,2,18),date(2022,1,31)])].plot.scatter(x = 'error_10m_windspeed_forecast', y = 'err', colormap = 'BrBG', c='day')\n",
    "df_compare_errors.loc[~np.isin(df_compare_errors.index.date, [date(2022,2,18),date(2022,1,31)])][['ws_100m_era5','error_10m_windspeed_forecast','err']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_fig = plot_feature_importance(model_era5.feature_importance_dataframe)\n",
    "feature_importance_fig.show()\n",
    "feature_importance_fig = plot_feature_importance(model_d2.feature_importance_dataframe)\n",
    "feature_importance_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store results\n",
    "Store timeseries as csv, metadata as yaml, model as ... and write an overview to pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = f'{datetime.utcnow():%Y%m%d_%H%M%S}_D2_weather_forecast_comparison_MDM_Total'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_artifacts(run_name, forecast, model, prediction_job, backtest_specs):\n",
    "    \"\"\"Write timeseries to csv and generate PDF of result\"\"\"\n",
    "    \n",
    "    # Create output dir\n",
    "    outdir = Path(f'output/{run_name}')\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "     \n",
    "    # Write forecast_df (includes realised)\n",
    "    forecast.to_csv(outdir / 'forecast.csv', compression='gzip')\n",
    "    \n",
    "    # Write model\n",
    "    model.save_model(outdir / \"model.json\")\n",
    "    \n",
    "    # Write meta data - prediction job and backtest parameters\n",
    "    # relevant prediction_job attributes\n",
    "    rel_attrs = ['id','name','model','quantiles']\n",
    "    rel_pj_dict={key:prediction_job[key] for key in rel_attrs}\n",
    "    with open(outdir / \"configs.yaml\", \"w\") as file:\n",
    "        documents = yaml.dump({**rel_pj_dict, **backtest_specs}, file)\n",
    "\n",
    "write_artifacts(run_name, forecast, model, pj, backtest_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_fname = '00.Evaluate_performance_using_Backtest_Pipeline'\n",
    "command=f\"jupyter nbconvert {nb_fname}.ipynb --to html --output results/{run_name}.html\"\n",
    "os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIFES",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
