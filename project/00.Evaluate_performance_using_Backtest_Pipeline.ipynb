{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17701ac5",
   "metadata": {},
   "source": [
    "# Imports and data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e72bdbd",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8413f5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873b5ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "from openstef.pipeline.train_create_forecast_backtest import train_model_and_forecast_back_test\n",
    "from openstef.metrics.figure import plot_feature_importance\n",
    "from openstef.data_classes.model_specifications import ModelSpecificationDataClass\n",
    "from openstef.data_classes.prediction_job import PredictionJobDataClass\n",
    "\n",
    "# Set working dir to location of this file\n",
    "os.chdir('.')\n",
    "\n",
    "# Set plotly as the default pandas plotting backend\n",
    "pd.options.plotting.backend = 'plotly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c3e987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "\n",
    "# This ensures Plotly output works in multiple places:\n",
    "# plotly_mimetype: VS Code notebook UI\n",
    "# notebook: \"Jupyter: Export to HTML\" command in VS Code\n",
    "# See https://plotly.com/python/renderers/#multiple-renderers\n",
    "pio.renderers.default = \"plotly_mimetype+notebook\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce309dc",
   "metadata": {},
   "source": [
    "## Load, pre-process, and visualize EMS measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ae85ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load inputs\n",
    "filename = Path(\"../.data/Middenmeer-150kV.csv\")\n",
    "\n",
    "measurements = pd.read_csv(filename, delimiter=\";\", decimal=\",\")\n",
    "measurements[\"Datetime\"] = pd.to_datetime(measurements[\"Datum\"] + \" \" + measurements[\"Tijd\"])\n",
    "measurements = measurements.set_index('Datetime').tz_localize('CET', ambiguous='NaT', nonexistent='NaT').tz_convert(\"UTC\")\n",
    "\n",
    "# Only keep relevant columns\n",
    "measurements = measurements.iloc[:,2:-1]\n",
    "\n",
    "# Sum the load\n",
    "measurements['Total'] = measurements.sum(axis=1)\n",
    "\n",
    "# By default, only a backtest is made for the total\n",
    "target_column = 'Total'\n",
    "\n",
    "measurements.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3427cbee",
   "metadata": {},
   "source": [
    "### Check the validity of the measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4c5810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all rows with a duplicate index\n",
    "measurements[measurements.index.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b57219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows with a NaT index.\n",
    "measurements = measurements[measurements.index.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41be34c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate that there are no duplicates left\n",
    "assert not(measurements.index.duplicated().any()), \"Duplicate indices have been found in the measurements dataframe.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd61386",
   "metadata": {},
   "source": [
    "## Load, pre-process, and visualize predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8adff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictors\n",
    "predictors = pd.read_csv('../.data/weather_apx_sji_sja_Middenmeer.csv', index_col=0, parse_dates=True)\n",
    "predictors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0038ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the validity of the predictors data\n",
    "assert not(predictors.duplicated().any()), \"Duplicate values have been found in the predictors dataframe.\"\n",
    "assert not(predictors.index.duplicated().any()), \"Duplicate indices have been found in the predictors dataframe.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e253e87",
   "metadata": {},
   "source": [
    "## Combine EMS measurements and predictors to get input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ae27a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenSTEF always expects a column called \"load\". This is the column it will predict.\n",
    "load = pd.DataFrame(dict(load=measurements.loc[:,target_column]))\n",
    "input_data = load.merge(predictors, left_index=True, right_index=True, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea385f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not(input_data.index.duplicated().any()), \"There are duplicate indices in the input data.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855e3d43",
   "metadata": {},
   "source": [
    "# Backtest configuration and execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98343eb1",
   "metadata": {},
   "source": [
    "## Configure training, prediction, and backtest specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38faa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define properties of training / prediction. We call this a 'prediction_job'.\n",
    "pj=PredictionJobDataClass(\n",
    "    id=1, # Does not matter in a backtest context\n",
    "    name='TestPrediction', # Does not matter in a backtest context\n",
    "    model='xgb',\n",
    "    quantiles=[0.10,0.30,0.50,0.70,0.90],\n",
    "    horizon_minutes=24*60, # TODO: Find out: Does this influence anything? Does this influence which lagged features are available at prediction time?\n",
    "    resolution_minutes=15,\n",
    "    forecast_type=\"demand\", # Note, this should become optional\n",
    "    lat = 1, # should become optional\n",
    "    lon = 1, # should become optional\n",
    "    # train_components=False, #should become optional\n",
    "    # model_type_group=None, # Note, this should become optional\n",
    "    # hyper_params={}, # Note, this should become optional\n",
    "    # feature_names=None, # Note, this should become optional\n",
    ")\n",
    "\n",
    "# The modelspecs do not do much if only an \"id\" is specified.\n",
    "modelspecs = ModelSpecificationDataClass(id=pj['id'])\n",
    "\n",
    "# Define backtest specs.\n",
    "backtest_specs = dict(n_folds=3, \n",
    "                      # The training horizon also decides for which forecast horizon, backtest forecasts are made.\n",
    "                      training_horizons=[0.25, 47.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e533aad4",
   "metadata": {},
   "source": [
    "## Perform the backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59a0ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the backtest\n",
    "forecast, models, train_data, validation_data, test_data = train_model_and_forecast_back_test(\n",
    "    pj,\n",
    "    modelspecs = modelspecs,\n",
    "    input_data = input_data,\n",
    "    **backtest_specs,\n",
    " )\n",
    "\n",
    "# If n_folds > 1, models is a list of models. In that case, only use the first model.\n",
    "if backtest_specs['n_folds'] > 1:\n",
    "    model=models[0]\n",
    "else:\n",
    "    model=models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa9c820",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 130)\n",
    "train_data[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff7aa36",
   "metadata": {},
   "source": [
    "# Evaluation of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7998c396",
   "metadata": {},
   "source": [
    "## Visualize forecasts for all horizons\n",
    "TODO:\n",
    "- Find out if there also is an in-sample fit that results from the backtest. (Ask JM.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43a415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d0091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import quantile_plotting\n",
    "\n",
    "for horizon in set(forecast.horizon):\n",
    "    quantile_plotting.plot_quantile_forecasts_and_realized(\n",
    "        realized=forecast.query(\"horizon == @horizon\")[\"realised\"],\n",
    "        forecast=forecast.query(\"horizon == @horizon\")[\"forecast\"],\n",
    "        quantiles=forecast.query(\"horizon == @horizon\")[[q for q in forecast.columns if q[:8] == \"quantile\"]],\n",
    "        horizon=horizon,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b26ece",
   "metadata": {},
   "source": [
    "## Compute and visualize performance measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95343d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast['err'] = forecast['realised'] - forecast['forecast']\n",
    "mae = forecast.pivot_table(index='horizon', values=['err'], aggfunc=lambda x: x.abs().mean())\n",
    "mae.index = mae.index.astype(str)\n",
    "mae_fig = mae.plot(kind='bar',\n",
    "          labels=dict(title='MAE',\n",
    "                      xaxis=dict(title='horizon'),\n",
    "                      yaxis=dict(title='MAE [MW]')))\n",
    "mae_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdaa2b6",
   "metadata": {},
   "source": [
    "## Visualize feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0753b380",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_fig = plot_feature_importance(model.feature_importance_dataframe)\n",
    "feature_importance_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304d3880",
   "metadata": {},
   "source": [
    "# Store results\n",
    "Store forecast timeseries as csv, metadata as yaml, model as json and write this notebook to html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2dda68",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = f'{datetime.utcnow():%Y%m%d_%H%M%S}_MDM_Total'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04f358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_artifacts(run_name, forecast, model, prediction_job, backtest_specs):\n",
    "    \"\"\"Write timeseries to csv and generate PDF of result\"\"\"\n",
    "    \n",
    "    # Create output dir\n",
    "    outdir = Path(f'output/{run_name}')\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "     \n",
    "    # Write forecast_df (includes realised)\n",
    "    forecast.to_csv(outdir / 'forecast.csv', compression='gzip')\n",
    "    \n",
    "    # Write model\n",
    "    model.save_model(outdir / \"model.json\")\n",
    "    \n",
    "    # Write meta data - prediction job and backtest parameters\n",
    "    # relevant prediction_job attributes\n",
    "    rel_attrs = ['id','name','model','quantiles']\n",
    "    rel_pj_dict={key: prediction_job[key] for key in rel_attrs}\n",
    "    with open(outdir / \"configs.yaml\", \"w\") as file:\n",
    "        documents = yaml.dump({**rel_pj_dict, **backtest_specs}, file)\n",
    "\n",
    "write_artifacts(run_name, forecast, model, pj, backtest_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc747e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nb_fname = '00.Evaluate_performace_using_Backtest_Pipeline'\n",
    "command=f\"jupyter nbconvert {nb_fname}.ipynb --to html --no-input --output results/{run_name}.html\"\n",
    "print(f\"Command to be executed: {command}.\")\n",
    "os.system(command)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
